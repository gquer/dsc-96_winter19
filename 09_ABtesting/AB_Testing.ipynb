{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "Goal: Get a feel for A/B-testing using Permutation Tests.\n",
    "\n",
    "One of the most common usages for A/B testing is for understanding the impact of changes to websites (e.g. website design changes, serving of different advertisements). For example, the website owner makes a small change to the layout of a webpage to a portion of the traffic and compares the user behavior on the old design against the user behavior on the new design. If there's a significant change in wanted behavior, the website owner will make the change on the entire website.\n",
    "\n",
    "Parameters that a website owner needs to consider when executing an A/B test are:\n",
    "* The portion of website traffic that should make up the treatment group (i.e. users who see the changed website).\n",
    "* How long the AB-test should last.\n",
    "* The impact and level of confidence in the result of the test to declare it a \"success\". That is, you should be comfortable that the result of your test bring enough positive change (e.g. increase in revenue) and that it's not due to random flucuations in website traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aaron's Awesome Amphibian Adoptions runs an AB test\n",
    "\n",
    "Suppose that the massively popular `aaronsawesomeamphibianadoptions.com` helps support its program by selling ads on its website. Aaron, the site owner, is considering making a change to his ad-targeting software in hopes of increasing fly sales at a sponsoring pet store. Every click on an advertisement leads to 2 cents in revenue.\n",
    "\n",
    "Consider the following scenarios for running Aaron's AB test. \n",
    "* Each AB test runs for 7 days, and website visits are recored up to the hour. Hours are recorded by \"number of hours since starting the AB test\" (between 0 and 167).\n",
    "* `PageA` denotes the current webpage (control), whereas `PageB` denotes the webpage with the change (treatment).\n",
    "* The column `clicked` denotes whether an ad was clicked on during the recorded visit (1) or not (0).\n",
    "\n",
    "### Exploring different splitting of treatment and control groups\n",
    "\n",
    "* Each of the files given below are results of the \"same\" hypothetical AB-test with different treatment/control group splits. \n",
    "\n",
    "Answer the following questions for each of the following AB-tests:\n",
    "* How many visitors to the webpage were there during the week of the AB test?\n",
    "* How many were shown `pageA` vs `pageB`?\n",
    "* Plot the number of visits for each page by the hour.\n",
    "* What is the average *click-rate* on the page overall? What about on `pageA`? `pageB`?\n",
    "    - click-rate is the number of ad-clicks per visitor on the page.\n",
    "* Plot the click-rate for each page by the hour.\n",
    "* Which page made more money during the week? How much more? (was the AB-test successful?)\n",
    "* Was the difference in click-rates bewteen the pages significant? \n",
    "    - Run a permutation test and calculate the p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 50-50 split\n",
    "\n",
    "Read in the file `data/AB_test_50_50.csv` and analyze the results of the AB test which splits website traffic evenly into treatment/control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 85-15 split\n",
    "\n",
    "Read in the file `data/AB_test_85_15.csv` and analyze the results of the AB test which splits website traffic evenly into treatment/control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 95-5 split\n",
    "\n",
    "Read in the file `data/AB_test_95_5.csv` and analyze the results of the AB test which splits website traffic evenly into treatment/control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 99-1 split\n",
    "\n",
    "Read in the file `data/AB_test_99_1.csv` and analyze the results of the AB test which splits website traffic evenly into treatment/control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "It's potentially quite costly to show the new webpage to a large number of users, given that the proposed change may be ineffective. One way to mitigate this risk is to shrink the treatment group. However, this decision is a trade-off for noisier results.\n",
    "\n",
    "* How big *does* our sample need to be?\n",
    "\n",
    "$$ n = 16\\sigma^2/\\Delta^2 $$\n",
    "\n",
    "Where: $\\sigma^2$ is the variance of the outcome metric \n",
    "* in case of binary outcome, $\\sigma^2 = p(1-p)$, where $p$ is the probability $X = 1$.\n",
    "*  $\\Delta$ is the sensitivity (amount you want to detect) at 80% power -- i.e. the difference in ($p_1 - p_2$). This parameter is based on how small of a variation you would like to meaningfully detect.\n",
    "\n",
    "* For a nice dashboard calculator: https://www.evanmiller.org/ab-testing/sample-size.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to stop an AB-test?\n",
    "\n",
    "Imagine your 1 week long AB test has gone live and you're anxiously watching the results every hour. A natural impulse would be to stop the AB test as soon as you see a positive result -- calling it successful. This is bad! The test period must be fixed ahead of time. To illustrate this, read in data for another AB test here: `data/AB_test_time_series.csv`. \n",
    "\n",
    "* Answer the same questions as above.\n",
    "* Now, for every hour in the AB test, pretend like you ended the AB test exactly then. Compute the significance levels of each of these tests, for each hour between 0 and 167. You should have a sequence of p-values. What do you see? Would've we come to the wrong conclusion by finishing the test early?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more discussion on these issues in AB testing, see: https://www.evanmiller.org/how-not-to-run-an-ab-test.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
