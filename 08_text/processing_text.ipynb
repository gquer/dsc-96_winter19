{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "The file `UCSD tweets.csv` has a small number of tweets from August and September 2018 that contained the term \"UCSD\".  Let's analyze them!\n",
    "\n",
    "### Steps\n",
    "\n",
    "Each step is explained in more detail below\n",
    "1. Open the CSV and explore the data\n",
    "2. Clean the data\n",
    "3. Count word frequency\n",
    "4. Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from datascience import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Open the CSV and explore the data\n",
    "\n",
    "### Steps:\n",
    "* load the data from `data/UCSD tweets.csv`\n",
    "* Examine the data.  How many tweets are there?  How long is the shortest tweet?  Longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "tweets = [x.strip() for x in open(\"data/UCSD tweets.csv\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean the data\n",
    "\n",
    "### Filter out tweets that don't have `UCSD` in the text\n",
    "\n",
    "The Twitter search matches on both username and tweet text.  We want just the ones that have a match in the tweet itself.  The result should be a new dataframe with the subset of matching tweets.\n",
    "\n",
    "* create functions to apply to your table and clean your data\n",
    "* use where clauses to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates\n",
    "\n",
    "See if any of the tweets have exactly the same text.  If so, are they true duplicates?  Does it make sense to remove them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Count Words\n",
    "\n",
    "We want to find out what the most frequent words are, so we need to split things up.  In text this is called tokenizing.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Make a single long string with all of the tweet text.  Make sure to put spaces between them!\n",
    "2. Split the tweets into a list of words using `.split()`\n",
    "3. Print out the first 20 words to make sure it looks like what you think it should.\n",
    "\n",
    "How many words are there all together?  How many distinct words? (remember `set()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short words\n",
    "\n",
    "Short words are really common, and aren't super helpful for comparing word count.  Usually it is best to remove what are called \"stop words\", which include things like \"of\", \"a\", \"in\", etc.  In this case we will just remove all words that are less than three charecters long.\n",
    "\n",
    "The result should be a new list of words.  How many total?  How many unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Count word frequency\n",
    "\n",
    "You can use a dictionary to create a categorical distribution of the words in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = 'Jack be nimble, Jack be quick, Jack jump over the candlestick'\n",
    "my_words = my_sentence.split()\n",
    "\n",
    "categorical_distribution = {} # empty dictionary\n",
    "for word in my_words:\n",
    "    if word in categorical_distribution:\n",
    "        categorical_distribution[word] = categorical_distribution[word] + 1\n",
    "    else:\n",
    "        categorical_distribution[word] = 1\n",
    "        \n",
    "categorical_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a categorical distribution of words for all tweets.  \n",
    "* Are you surprised by the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.b. Tokenize again (with NLTK this time)\n",
    "\n",
    "Why is UCSD only in 18?  \n",
    "\n",
    "Because of `@UCSD` and similar.  \n",
    "\n",
    "Tokenizing (like most things) is harder than it looks at first!  \n",
    "\n",
    "Generally, a good solution is to use a tool built for the job rather than rolling your own.  In this case, we will use the Python package Natural Language ToolKit, NLTK.  \n",
    "\n",
    "You may need to install NLTK and also download an English language corpus.  If so, do this in the terminal:\n",
    "\n",
    "```\n",
    "pip install --user nltk\n",
    "```\n",
    "\n",
    "Then in Jupyter run this once:\n",
    "```\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```\n",
    "\n",
    "Run the code below to use NLTK's tokenizer, and then repeat the process of removing short words and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = ... # pass in a string consisting of all tweets\n",
    "\n",
    "wordList = tokenize.word_tokenize(allText)\n",
    "len(wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.b. Count (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment with NLTK\n",
    "\n",
    "What is sentiment?  Why do we care?\n",
    "\n",
    "Will need to run once:\n",
    "```\n",
    "nltk.download('vader_lexicon')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(\"Good test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = Table.read_table('data/UCSD tweets.csv')\n",
    "tweetList = tweets.column('text') # a list of strings, with each string a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetSentiments = []\n",
    "\n",
    "for tweet in tweetList:\n",
    "    tweetSentiment = sid.polarity_scores(tweet)\n",
    "    tweetSentiment['text'] = tweet\n",
    "    tweetSentiments.append(tweetSentiment)\n",
    "    \n",
    "tweetSentiments # a list of dictionaries, with text of tweets and sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.from_records(tweetSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetSentimentDf.sort_values('compound')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "* load the file of \"internet research agency\" tweets (a small sample) and explore!\n",
    "    - `data/ira.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
