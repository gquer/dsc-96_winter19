{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Supervised Learning\n",
    "\n",
    "Today, we'll learn about making prediction when we have an example dataset at our disposal. In class, we've learned about using both the *graph of averages* and the *regression line* to make a prediction about one characteristic based on another characteristic. When the characteristics are linearly associated, these two methods are equivalent.\n",
    "\n",
    "The techniques we'll use today are more general, non-linear techniques:\n",
    "1. *k nearest neighbors* (k-NN), \n",
    "2. *decision tree models*\n",
    "3. *random forests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "from datascience import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction functions: regression\n",
    "\n",
    "Suppose we want to predict some characteristic $y$ from a dataset with characteristics (columns) named $x_1,\\ldots,x_n$. To do this, we look for a *prediction function* $y = f(x_1,\\ldots,x_n)$ that takes our known characteristics to a best guess for $y$.\n",
    "\n",
    "The regression line is a *linear prediction function* $f(x) = mx + b$ that predicts the value $y = mx + b$ for a given input $x$. \n",
    "\n",
    "In the plot below, the regression line predicts a value of $y \\approx 1.2$ at the input $x = 101$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_regression\n",
    "y, X = make_regression(100, 1, noise=30)\n",
    "\n",
    "Table().with_columns('X',X, 'y', y.reshape(-1)).scatter(0, fit_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot below depicts the heights and weights of nba players in 2013. The regression line predicts a rookie (not in our dataset) of height 85 inches should weight approximately 260 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = Table.read_table('data/nba2013.csv')\n",
    "nba.select('Height', 'Weight').scatter(0, fit_line=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the prediction function\n",
    "\n",
    "There are many ways to fit a prediction function to a dataset, so how do we know which is the 'correct' way?\n",
    "\n",
    "For example, are height and weight in the nba dataset really linearly associated? Should've we fit the data with a non-linear prediction function?\n",
    "\n",
    "Below, we calculate the graph of averages prediction function and see that the prediction curve we get does *not* look linear near the tails of the height distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_of_averages(x):\n",
    "    nearby = nba.where('Height', are.between_or_equal_to(x - 1, x + 1))\n",
    "    return np.mean(nearby.column('Weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goa_out = nba.select('Height', 'Weight').with_column('predictions', nba.apply(graph_of_averages, 'Height'))\n",
    "goa_out.scatter(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know which prediction function is best? Does the non-linear predictor only look better because there are fewer data points at the ends of the distribution, and the plot is overly concerned with only a few examples?\n",
    "\n",
    "This concept is known as the bias-variance trade-off: you want your predictor to predict the correct value, without learning the random noise in the data (this is called *overfitting*). You want your predictor to learn the *pattern*, not the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting your data for accuracy\n",
    "\n",
    "To search for the best predictor while avoiding overfitting your data, you want to split your data set into *training set* and a *test set*. \n",
    "\n",
    "* The training set is used to *train* your predictor.\n",
    "* The test set is used to *test* the goodness-of-fit of your trained predictor.\n",
    "\n",
    "By leaving out data for testing, you are ensuring that the best predictor you find is the one that best learns the patterns in the data, and not the data points themselves!\n",
    "\n",
    "## The machine learning training pipeline:\n",
    "\n",
    "Scikit-Learn as functions that help us do this. Let's look at the problem of predicting heights from weights in the nba dataset, and find the best predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nba.column('Height').reshape(-1,1) # input attributes\n",
    "y = nba.column('Weight') # what we want to predict\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking three possible predictors\n",
    "\n",
    "From the scikit library, we'll try the following three predictors:\n",
    "1. k-nearest-neighbors regression: find the closest k data points from a given data point, and average their values. That average is the prediction.\n",
    "2. linear regression: using the line of 'best fit' in the training set to predict values in the test set.\n",
    "3. support-vector regression: find a non-linear curve of 'best fit' to the training set to predict values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit 5-nearest neighbors regressor\n",
    "knr = KNeighborsRegressor()\n",
    "knr.fit(X_train, y_train)\n",
    "\n",
    "# fit linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# fit support-vector-regressor\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each prediction function is fit on the training set, we'll evaluate the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table().with_columns(\n",
    "    'X test', X_test.flatten(),\n",
    "    'y test', y_test,\n",
    "    'prediction knr', knr.predict(X_test),\n",
    "    'prediction lr', lr.predict(X_test),\n",
    "    'prediction svr', svr.predict(X_test)\n",
    ").sort('X test')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(t.column(0), t.column(1))\n",
    "plt.plot(t.column(0), t.column(2))\n",
    "plt.plot(t.column(0), t.column(3))\n",
    "plt.plot(t.column(0), t.column(4))\n",
    "plt.legend(['k-nn', 'linear', 'svm']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit learn library also scores quality of the predictions. In the case of regression, the score is given by \n",
    "\n",
    "$$R^2 = (1 - u/v)$$\n",
    "\n",
    "where \n",
    "1. $u$ is the sum of the squares of the errors: `u = np.sum((y_test - y_pred) ** 2)`\n",
    "2. $v$ is the sum of the squares of the deviation from the average: `v = np.sum((y_test - y_test.mean()) ** 2)`\n",
    "\n",
    "Thus, the best possilbe score is 1.0 (when the errors are 0). A constant model that always predictions the mean value of the true values (disregarding input features) would have a value of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"k-nn score:\\t\\t\\t\\t%f\" % knr.score(X_test, y_test),\n",
    "    \"linear regression score:\\t\\t%f\" % lr.score(X_test, y_test),\n",
    "    \"support vector regression score:\\t%f\" % svr.score(X_test, y_test),\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the linear model performed best on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Up until now, we've focused on *regression* problems, where our prediction function is trying to predict real numbers in its output. However, what if our prediction function $y = f(x)$ is trying to learn a finite number of labels, instead? This is problem is called the *classification* problem.\n",
    "\n",
    "As an example, suppose we are trying to learn the nba players position based on `Height` and `Weight` -- that is, we are trying to learn a prediction function:\n",
    "\n",
    "$$y = f(h, w)$$\n",
    "\n",
    "Where $h$ is a players height, $w$ is a players weight, and $y$ is one of the values {guard, forward, center}. Much of the process of fitting our prediction function will remain the same, however the learning algorithms and the way we evaluate them will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nba.select('Height', 'Weight').values\n",
    "y = nba.column('Position')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = DecisionTreeClassifier(max_depth=8)\n",
    "tr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Table().with_columns(\n",
    "    'Height', X_test[:,0],\n",
    "    'Weight', X_test[:,1],\n",
    "    'true', y_test,\n",
    "    'pred', tr.predict(X_test),\n",
    "    'correct', np.where(y_test == tr.predict(X_test), 'correct', 'incorrect')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.scatter(0,1, colors='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.scatter(0,1, colors='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.scatter(0,1, colors='correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score method computes the *accuracy* of the classifier: the proportion of instances were correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy via the score function\n",
    "tr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy using the definition\n",
    "np.mean(tr.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the decision tree logic by looking inside the fit classifier. The function returns the number of labels in the training set seen for that branch of the tree. The prediction function would predict the most common value in each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from print_tree import tree_to_code\n",
    "tree_to_code(tr, ['Height', 'Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Change the parameters for the descision tree (e.g. set max-depth higher). Which choice of parameters give the best accuracy on the test set?\n",
    "* Look into the predictions more closely:\n",
    "    - which positions are misclassified most?\n",
    "    - who are the players being misclassified?\n",
    "* Try other classification algorithms, such as `RandomForestClassifier` and `KNeighborsClassifier` imported below.\n",
    "* Use the `nba2016` dataset, and try to predict player positions based on player statistics.\n",
    "* Use the SDPD traffic stops datasets. \n",
    "    - Can you predict the age/gender/ethnicity based on other factors? (which are regression? classification?)\n",
    "    - Can you predict who will be search or arrested, based on attributes of the stopped driver?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # Votes of many decision trees\n",
    "from sklearn.neighbors import KNeighborsClassifier # Votes of nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
